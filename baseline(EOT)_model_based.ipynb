{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0678bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,k,d,size=8,danger=[7,1],goal=[4,5],wall=[2,5],coins=[(1,6),(4,2),(5,5)], horizon=50, noise=0.1):\n",
    "        self.noise = noise\n",
    "        self.k=k\n",
    "        self.d=d\n",
    "        self.size = size\n",
    "        self.horizon = horizon\n",
    "        self.goal = tuple(goal)\n",
    "        self.danger = tuple(danger)\n",
    "        self.wall = tuple(wall)\n",
    "        self._init_coins = tuple(map(tuple, coins)) \n",
    "        self.coins = set(self._init_coins)\n",
    "        self.collected_coins = set()\n",
    "        self.done = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.done = 0\n",
    "        self.pos = (0,7)\n",
    "        self.t = 0\n",
    "        self.collected = 0\n",
    "        self.collected_coins = set()\n",
    "        self.coins = set(self._init_coins)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, intended_action):\n",
    "        probs = np.full(4, 0.03)\n",
    "        probs[intended_action] = 0.91\n",
    "        action = np.random.choice(4, p=probs)\n",
    "        x, y = self.pos\n",
    "        if action == 0: x = max(0, x-1)       # up\n",
    "        if action == 1: x = min(self.size-1, x+1) # down\n",
    "        if action == 2: y = max(0, y-1)       # left\n",
    "        if action == 3: y = min(self.size-1, y+1) # right\n",
    "        if((x,y)!=(self.wall)):\n",
    "            self.pos = (x,y)\n",
    "        if self.pos in self.coins:\n",
    "            self.collected += 1\n",
    "            self.collected_coins.add(self.pos)\n",
    "            self.coins.remove(self.pos)\n",
    "        self.t += 1\n",
    "        self.done = ((self.t >= self.horizon)or (self.pos==self.goal) or (self.pos==self.danger))\n",
    "        return self.pos, self.done\n",
    "        \n",
    "\n",
    "    def get_feedback_and_features(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal)) - 5*(self.t-14)/self.horizon + 5*36/50)\n",
    "#         scaled_reward = 10*(1-math.exp(-true_reward/5))/(1+math.exp(-true_reward/5))\n",
    "        scaled_reward = 2*true_reward\n",
    "        ## now we quantize it into k bins\n",
    "        edges = np.linspace(0,32.92,self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if(edges[i]<=scaled_reward and scaled_reward<edges[i+1]):\n",
    "                feedback = i\n",
    "                break\n",
    "                \n",
    "        # label noise\n",
    "        probs = [0.0] * self.k\n",
    "        probs[feedback] = 1-self.noise+self.noise/self.k\n",
    "        rem = 1-probs[feedback]\n",
    "        rem_distributed = rem / (self.k - 1)\n",
    "        for i in range(self.k):\n",
    "            if probs[i] == 0.0:\n",
    "                probs[i] = rem_distributed\n",
    "   \n",
    "        feedback_list = [i for i in range(self.k)]\n",
    "        feedback_given = np.random.choice(feedback_list,p=probs)\n",
    "        return feedback_given,self._features()\n",
    "    \n",
    "    def true_return(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal)) - 5*(self.t-14)/self.horizon + 5*36/50)\n",
    "        scaled_reward = 2*true_reward\n",
    "        edges = np.linspace(0,32.92,self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if(edges[i]<=scaled_reward and scaled_reward<edges[i+1]):\n",
    "                feedback = i\n",
    "                break\n",
    "        return feedback\n",
    "                \n",
    "    \n",
    "    def _features(self):\n",
    "        \"\"\"return trajectory features phi(tau)\"\"\"\n",
    "        x, y = self.pos\n",
    "        xg, yg = self.goal\n",
    "        xd, yd = self.danger\n",
    "        dist_to_goal = abs(x-xg) + abs(y-yg)\n",
    "        dist_to_danger = abs(x-xd) + abs(y-yd)\n",
    "        at_danger = int(self.pos == self.danger)\n",
    "        at_goal = int(self.pos == self.goal and (at_danger==0))\n",
    "        coin_indicator = [int(c in self.collected_coins) for c in self._init_coins]\n",
    "        return np.array([dist_to_goal, dist_to_danger, at_goal, at_danger] + coin_indicator, dtype=float)\n",
    "        \n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, grid_size, action_dim):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_dim = grid_size * grid_size\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = np.ones((self.state_dim, self.action_dim))\n",
    "    \n",
    "    def state_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "    \n",
    "    def act(self, state):\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def grad_log_prob(self, state, action):\n",
    "        \"\"\"Return (state_index, grad_row) with grad_row shape (action_dim,)\n",
    "           grad_row[j] = 1{j==action} - pi(j|s)\"\"\"\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        grad_row = -probs.copy()\n",
    "        grad_row[action] += 1.0\n",
    "        return s_idx, grad_row\n",
    "    \n",
    "\n",
    "\n",
    "class RewardModel:\n",
    "    def __init__(self, k,d,C=0.0):\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.W = np.zeros((k,d))\n",
    "        self.C = C\n",
    "        \n",
    "    def estimate_W(self, X, Y, reg=1e-3):\n",
    "        n, d = X.shape\n",
    "        W = cp.Variable((self.k, self.d))  # optimization variable, not vectorized, shape(k,d)\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            phi = X[i]  \n",
    "            yi = int(Y[i])\n",
    "            logits = W @ phi \n",
    "            ## yi cannot be more than (k-1), so if assigning deterministic rewards without crafting W*, be careful\n",
    "            loss += -(logits[yi] - cp.log_sum_exp(logits))\n",
    "        \n",
    "        loss = loss/n + reg*cp.norm(W, \"fro\")**2\n",
    "        prob = cp.Problem(cp.Minimize(loss))\n",
    "        prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "        self.W = W.value\n",
    "        return self.W\n",
    "    \n",
    "    def reward_probabilities(self, phi):\n",
    "        \"\"\"estimate P(y|tau)\"\"\"\n",
    "        logits = self.W @ phi\n",
    "        logits-= np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        return exp_logits/np.sum(exp_logits)\n",
    "    \n",
    "    def reward_estimate(self,phi):\n",
    "        \"\"\"returns the average, expected reward given the reward probabilities\"\"\"\n",
    "        return np.sum(np.array([i*self.reward_probabilities(phi)[i] for i in range(self.k)]))\n",
    "    \n",
    "    def optimistic_reward(self, phi, n_samples):\n",
    "        \"\"\"\n",
    "        optimism term included\n",
    "        \"\"\"\n",
    "        base = self.reward_estimate(phi)\n",
    "        n = max(n_samples, 1)\n",
    "        bonus = self.C / np.sqrt(n)\n",
    "        optimistic = base + bonus\n",
    "        return min(optimistic, self.k - 1)\n",
    "\n",
    "\n",
    "###----------Traning loop-------------###\n",
    "def train(N=20,m=50,k=6,eta=0.1,epsilon=0.1,grid_size=8,danger=[7,1],goal=[0,7], wall=[2,5] ,horizon=50,coins=None,seed=0,noise=0.1):\n",
    "    np.random.seed(seed)\n",
    "    queries = 0\n",
    "    steps = 0\n",
    "    if coins is None:\n",
    "        coins=[(1,6),(4,2),(5,5)]\n",
    "    print(\"hi\")\n",
    "    d = 4+len(coins)\n",
    "#     W_true = generate_W_true(k,d)\n",
    "    env = GridWorld(k,d,size=grid_size, danger=danger, goal=goal, wall=wall, coins=coins, horizon=horizon, noise=noise)\n",
    "    policy = Policy(grid_size=grid_size, action_dim=4)\n",
    "    \n",
    "    ## initialize weights w_0\n",
    "    reward_model = RewardModel(k, d, C=10.0)\n",
    "    reward_model.W = np.zeros((k,d))\n",
    "    \n",
    "    all_data_X, all_data_Y = [], []\n",
    "    avg_true_rewards,avg_coins,avg_est_rewards = [], [], []\n",
    "    flag = 0\n",
    "    for n in range(N):\n",
    "        if flag: break\n",
    "        print(n)\n",
    "        avg_true_reward_this_iter = 0\n",
    "        avg_est_reward_this_iter = 0\n",
    "        if(n<100):\n",
    "            G_range = 10\n",
    "        elif (n>100 ):\n",
    "            G_range = 100\n",
    "\n",
    "        for g in range(G_range):\n",
    "            steps+=1\n",
    "            rollout_trajectories = []\n",
    "            avg_true_reward_this_iter = 0\n",
    "            avg_est_reward_this_iter = 0\n",
    "            rewards = []\n",
    "            for i in range(m): ## sample trajectories under current policy pi to approiximate the theoretical expectation\n",
    "                s = env.reset()\n",
    "                traj = {\"states\": [], \"actions\": [], \"steps\":0, \"coins\":0}\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    a, _ = policy.act(s)\n",
    "                    traj[\"states\"].append(s)\n",
    "                    traj[\"actions\"].append(a)\n",
    "                    s, done = env.step(a)\n",
    "            \n",
    "                traj[\"steps\"] = env.horizon\n",
    "                traj[\"coins\"] = env.collected\n",
    "                y, phi = env.get_feedback_and_features()\n",
    "                phi = np.array(phi, dtype=float)   \n",
    "                rollout_trajectories.append((traj, phi, y))\n",
    "                rewards.append(env.true_return())\n",
    "#             if(np.mean(rewards)>31):\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "\n",
    "                \n",
    "            ## now with these m rollouts, approximate the expectation of estimated reward under policy pi\n",
    "            grad_theta = np.zeros_like(policy.theta)\n",
    "            n_samples = max(len(all_data_X), 1)\n",
    "            R_hats = [reward_model.optimistic_reward(phi, n_samples) for _, phi, _ in rollout_trajectories]\n",
    "            b = float(np.mean(R_hats))  # baseline\n",
    "                        \n",
    "            for (traj,phi_tau,y), r_hat in zip(rollout_trajectories,R_hats):\n",
    "                \n",
    "                avg_est_reward_this_iter+=r_hat/len(rollout_trajectories)\n",
    "#                 avg_true_reward_this_iter+=y/len(rollout_trajectories)\n",
    "                temp = r_hat-b\n",
    "                    \n",
    "                for state,action in zip(traj[\"states\"],traj[\"actions\"]):\n",
    "                    s_idx, grad_row = policy.grad_log_prob(state, action)\n",
    "                    grad_theta[s_idx] += grad_row*(temp)\n",
    "\n",
    "            grad_theta = grad_theta/len(rollout_trajectories)\n",
    "            policy.theta += eta*grad_theta\n",
    "            avg_true_reward_this_iter = np.mean(rewards)\n",
    "            \n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a,_ = policy.act(s)\n",
    "            s,done = env.step(a)\n",
    "        y,phi = env.get_feedback_and_features()\n",
    "        all_data_X.append(phi)\n",
    "        all_data_Y.append(y)\n",
    "        reward_model.W = reward_model.estimate_W(np.array(all_data_X),np.array(all_data_Y), reg = 1e-3)\n",
    "         \n",
    "        traj,phi,y = rollout_trajectories[-1]\n",
    "\n",
    "        coins_this_iter = traj[\"coins\"]\n",
    "        \n",
    "        ## storing some info\n",
    "        avg_est_rewards.append(avg_est_reward_this_iter)\n",
    "        avg_true_rewards.append(avg_true_reward_this_iter)\n",
    "        avg_coins.append(coins_this_iter)\n",
    "        \n",
    "        # update estimate of weight matrix W\n",
    "        reward_model.W = reward_model.estimate_W(np.array(all_data_X),np.array(all_data_Y), reg = 1e-3)\n",
    "\n",
    "        print(f\"Iter {n:02d}: avg_estimated_reward={avg_est_rewards[-1]:.3f}, avg_true_reward={avg_true_rewards[-1]:.3f},coins_this_episode={avg_coins[-1]:.2f}\")\n",
    "    \n",
    "    return policy, reward_model, avg_true_rewards, avg_est_rewards, reward_model.W,steps,queries\n",
    "\n",
    "\n",
    "trained_policy, trained_reward_model, avg_true, avg_est, W,steps,queries = train(N=150,m=20,k=6,eta=0.5,epsilon=1e-2,grid_size=8,danger=[7,1],goal=[4,5], wall=[2,5],coins=[(1,6),(4,2),(5,5)],seed=2,noise=0.1)\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = trained_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gridworld_path(env, states):\n",
    "    size = env.size\n",
    "    xs = [s[1] for s in states]\n",
    "    ys = [s[0] for s in states]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    for i in range(size + 1):\n",
    "        ax.axhline(i - 0.5, linewidth=0.6)\n",
    "        ax.axvline(i - 0.5, linewidth=0.6)\n",
    "    ax.set_xlim(-0.5, size - 0.5)\n",
    "    ax.set_ylim(-0.5, size - 0.5)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.plot(xs, ys, marker='o', linewidth=1.5, label='path')\n",
    "    if states:\n",
    "        ax.scatter(xs[0], ys[0], marker='o', s=100, label='start')\n",
    "        ax.scatter(xs[-1], ys[-1], marker='D', s=100, label='end')\n",
    "\n",
    "    gx, gy = env.goal\n",
    "    dx, dy = env.danger\n",
    "    ax.scatter(gy, gx, marker='^', s=140, label='goal')\n",
    "    ax.scatter(dy, dx, marker='X', s=140, label='danger')\n",
    "\n",
    "    wx, wy = env.wall\n",
    "    ax.scatter(wy, wx, marker='s', s=140, label='wall')\n",
    "\n",
    "    init_coins = list(env._init_coins)\n",
    "    remaining = list(env.coins)\n",
    "    collected = list(env.collected_coins)\n",
    "\n",
    "    if init_coins:\n",
    "        ax.scatter(\n",
    "            [c[1] for c in init_coins],\n",
    "            [c[0] for c in init_coins],\n",
    "            marker='s', s=80, alpha=0.25, label='coin tiles (all)'\n",
    "        )\n",
    "\n",
    "    if remaining:\n",
    "        ax.scatter(\n",
    "            [c[1] for c in remaining],\n",
    "            [c[0] for c in remaining],\n",
    "            marker='s', s=80, label='coins (remaining)'\n",
    "        )\n",
    "\n",
    "    if collected:\n",
    "        ax.scatter(\n",
    "            [c[1] for c in collected],\n",
    "            [c[0] for c in collected],\n",
    "            marker='*', s=180, label='coins (collected)'\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(range(size))\n",
    "    ax.set_yticks(range(size))\n",
    "    ax.set_xlabel('y (column)')\n",
    "    ax.set_ylabel('x (row)')\n",
    "    ax.set_title('GridWorld trajectory')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "done = 0\n",
    "traj = {\"states\": [], \"actions\": [], \"steps\": 0, \"coins\": 0}\n",
    "env = GridWorld(2,7,size=8, danger=[7,1],goal=[4,5],wall=[2,5],coins=[(1,6),(4,2),(5,5)], horizon=50) ##set value of k here\n",
    "\n",
    "s = env.reset() \n",
    "c = set([(1,6),(4,2),(5,5)])\n",
    "while not done:\n",
    "    a, _ = pi.act(s)\n",
    "    traj[\"states\"].append(s)\n",
    "    traj[\"actions\"].append(a)\n",
    "    s, done = env.step(a)\n",
    "    if (s in c):\n",
    "        print(f\"coin found at position: {s}\")\n",
    "        c.remove(s)\n",
    "    if (s == env.goal):\n",
    "        print(\"goal reached!\")\n",
    "    if (s == env.danger):\n",
    "        print(\"danger zone\")\n",
    "        \n",
    "print(env.t)\n",
    "\n",
    "plot_gridworld_path(env, traj[\"states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12a089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_01 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.1\n",
    "    )\n",
    "    all_curves_01.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_01 = np.stack(all_curves_01, axis=0)   \n",
    "\n",
    "mean_curve_01 = all_curves_01.mean(axis=0)\n",
    "std_curve_01  = all_curves_01.std(axis=0)\n",
    "\n",
    "x_01 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x, mean_curve_01, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x, mean_curve_01 - std_curve_01, mean_curve_01 + std_curve_01,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a9dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_05 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.5\n",
    "    )\n",
    "    all_curves_05.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_05 = np.stack(all_curves_05, axis=0)   \n",
    "\n",
    "mean_curve_05 = all_curves_05.mean(axis=0)\n",
    "std_curve_05  = all_curves_05.std(axis=0)\n",
    "\n",
    "x_05 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x_05, mean_curve_05, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x_05, mean_curve_05 - std_curve_05, mean_curve_05 + std_curve_05,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10f0ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_08 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.8\n",
    "    )\n",
    "    all_curves_08.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_08 = np.stack(all_curves_08, axis=0)   \n",
    "\n",
    "mean_curve_08 = all_curves_08.mean(axis=0)\n",
    "std_curve_08  = all_curves_08.std(axis=0)\n",
    "\n",
    "x_08 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x_08, mean_curve_08, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x_08, mean_curve_08 - std_curve_08, mean_curve_08 + std_curve_08,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aef39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves_08 = np.stack(all_curves_08, axis=0)   \n",
    "\n",
    "mean_curve_08 = all_curves_08.mean(axis=0)\n",
    "std_curve_08  = all_curves_08.std(axis=0)\n",
    "\n",
    "x_08 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x_08, mean_curve_08, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x_08, mean_curve_08 - std_curve_08, mean_curve_08 + std_curve_08,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stack and compute mean/std for each noise level\n",
    "all_curves_01 = np.stack(all_curves_01, axis=0)\n",
    "all_curves_05 = np.stack(all_curves_05, axis=0)\n",
    "all_curves_08 = np.stack(all_curves_08, axis=0)\n",
    "\n",
    "mean_01 = all_curves_01.mean(axis=0)\n",
    "std_01  = all_curves_01.std(axis=0)\n",
    "\n",
    "mean_05 = all_curves_05.mean(axis=0)\n",
    "std_05  = all_curves_05.std(axis=0)\n",
    "\n",
    "mean_08 = all_curves_08.mean(axis=0)\n",
    "std_08  = all_curves_08.std(axis=0)\n",
    "\n",
    "N_EPISODES = mean_01.shape[0]   # or whatever you used before\n",
    "x = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# noise = 0.1\n",
    "plt.plot(x, mean_01, label=\"noise = 0.1\")\n",
    "plt.fill_between(x, mean_01 - std_01, mean_01 + std_01, alpha=0.15)\n",
    "\n",
    "# noise = 0.5\n",
    "plt.plot(x, mean_05, label=\"noise = 0.5\")\n",
    "plt.fill_between(x, mean_05 - std_05, mean_05 + std_05, alpha=0.15)\n",
    "\n",
    "# noise = 0.8\n",
    "plt.plot(x, mean_08, label=\"noise = 0.8\")\n",
    "plt.fill_between(x, mean_08 - std_08, mean_08 + std_08, alpha=0.15)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"Average feedback per episode (mean ± 1 std over seeds)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b07264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('mean_01.npy', mean_01)\n",
    "np.save('std_01.npy', std_01)\n",
    "np.save('mean_05.npy', mean_05)\n",
    "np.save('std_05.npy', std_05)\n",
    "np.save('mean_08.npy', mean_08)\n",
    "np.save('std_08.npy', std_08)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "595",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
