{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7580d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,k,d,noise,size=8,danger=[7,1],goal=[4,5],wall=[2,5],coins=[(1,6),(4,2),(5,5)], horizon=50, feedback_every=10):\n",
    "        self.noise = noise\n",
    "        self.k=k\n",
    "        self.d=d\n",
    "        self.size = size\n",
    "        self.horizon = horizon\n",
    "        self.goal = tuple(goal)\n",
    "        self.danger = tuple(danger)\n",
    "        self.wall = tuple(wall)\n",
    "        self._init_coins = tuple(map(tuple, coins)) \n",
    "        self.coins = set(self._init_coins)\n",
    "        self.collected_coins = set()\n",
    "        self.done = 0\n",
    "        self.feedback_every = feedback_every\n",
    "        self.gamma = 1\n",
    "        self.previous_pos = None\n",
    "    \n",
    "    def reset(self, random=False):\n",
    "        self.done = 0\n",
    "        self.t = 0\n",
    "        self.collected = 0\n",
    "        self.collected_coins = set()\n",
    "        self.coins = set(self._init_coins)\n",
    "        self.previous_pos = None\n",
    "\n",
    "        if random:\n",
    "            while True:\n",
    "                r = np.random.randint(0, self.size)\n",
    "                c = np.random.randint(0, self.size)\n",
    "                potential_start = (r, c)\n",
    "                \n",
    "                # Check if the potential start is one of the terminal/wall states\n",
    "                if potential_start not in {self.goal, self.danger, self.wall}:\n",
    "                    self.pos = potential_start\n",
    "                    break\n",
    "        else:\n",
    "            self.pos = (7, 0)\n",
    "\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, intended_action):\n",
    "        self.previous_pos = self.pos\n",
    "        probs = np.full(4, 0.03)\n",
    "        probs[intended_action] = 0.91\n",
    "        action = np.random.choice(4, p=probs)\n",
    "        x, y = self.pos\n",
    "        if action == 0: x = max(0, x-1)       # up\n",
    "        if action == 1: x = min(self.size-1, x+1) # down\n",
    "        if action == 2: y = max(0, y-1)       # left\n",
    "        if action == 3: y = min(self.size-1, y+1) # right\n",
    "        if((x,y)!=(self.wall)):\n",
    "            self.pos = (x,y)\n",
    "        if self.pos in self.coins:\n",
    "            self.collected += 1\n",
    "            self.collected_coins.add(self.pos)\n",
    "            self.coins.remove(self.pos)\n",
    "        self.t += 1\n",
    "        self.done = ((self.t >= self.horizon)or (self.pos==self.goal) or (self.pos==self.danger))\n",
    "        return self.pos, self.done\n",
    "\n",
    "    def get_feedback(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "\n",
    "        true_reward = (1-(self.pos==self.danger))*(weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal)) - 5*(self.t-14)/self.horizon + 5*36/50)\n",
    "\n",
    "        scaled_reward = 2*true_reward\n",
    "\n",
    "        ## now we quantize it into k bins\n",
    "        edges = np.linspace(0,32.92,self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if(edges[i]<=scaled_reward and scaled_reward<edges[i+1]):\n",
    "                feedback = i\n",
    "                break\n",
    "\n",
    "       # label noise\n",
    "        probs = [0.0] * self.k\n",
    "        probs[feedback] = 1-self.noise+self.noise/self.k\n",
    "        rem = 1-probs[feedback]\n",
    "        rem_distributed = rem / (self.k - 1)\n",
    "        for i in range(self.k):\n",
    "            if probs[i] == 0.0:\n",
    "                probs[i] = rem_distributed\n",
    "   \n",
    "        feedback_list = [i for i in range(self.k)]\n",
    "        feedback_given = np.random.choice(feedback_list,p=probs)\n",
    "        return feedback_given, feedback\n",
    "                \n",
    "    \n",
    "    def _features(self):\n",
    "        \"\"\"return trajectory features phi(tau)\"\"\"\n",
    "        x, y = self.pos\n",
    "        xg, yg = self.goal\n",
    "        xd, yd = self.danger\n",
    "        dist_to_goal = abs(x-xg) + abs(y-yg)\n",
    "        dist_to_danger = abs(x-xd) + abs(y-yd)\n",
    "        at_danger = int(self.pos == self.danger)\n",
    "        at_goal = int(self.pos == self.goal and (at_danger==0))\n",
    "        coin_indicator = [int(c in self.collected_coins) for c in self._init_coins]\n",
    "        return np.array([dist_to_goal, dist_to_danger, at_goal, at_danger] + coin_indicator, dtype=float)\n",
    "        \n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, grid_size, action_dim):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_dim = grid_size * grid_size\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = np.ones((self.state_dim, self.action_dim))\n",
    "    \n",
    "    def state_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "    \n",
    "    def act(self, state):\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def grad_log_prob(self, state, action):\n",
    "        \"\"\"Return (state_index, grad_row) with grad_row shape (action_dim,)\n",
    "           grad_row[j] = 1{j==action} - pi(j|s)\"\"\"\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        grad_row = -probs.copy()\n",
    "        grad_row[action] += 1.0\n",
    "        return s_idx, grad_row\n",
    "\n",
    "def train(feedback_every, noise, seed, m=10, k=6, eta=0.1, grid_size=8, num_coins=3):\n",
    "    np.random.seed(seed)\n",
    "    d = 4 + num_coins\n",
    "\n",
    "    env = GridWorld(k=k, d=d, noise=noise, feedback_every=feedback_every)\n",
    "    policy = Policy(grid_size=grid_size, action_dim=4)\n",
    "    \n",
    "    avg_true_rewards, avg_coins, avg_length = [], [], []\n",
    "\n",
    "    for g in range(3000):\n",
    "        theta_old = policy.theta.copy()\n",
    "        rollout_trajectories = []\n",
    "\n",
    "        true_rewards_this_iter = []\n",
    "\n",
    "        for i in range(m): ## sample trajectories under current policy pi to approiximate the theoretical expectation\n",
    "            s = env.reset()\n",
    "            traj = {\"states\": [], \"actions\": [], \"human_rewards\": [], \"true_rewards\": [], \"step_rewards\": [], \"steps\":0, \"coins\":0}\n",
    "            done = False\n",
    "            previous_feedback = 0\n",
    "\n",
    "            while not done:\n",
    "                a, _ = policy.act(s)\n",
    "                traj[\"states\"].append(s)\n",
    "                traj[\"actions\"].append(a)\n",
    "                s, done = env.step(a)\n",
    "\n",
    "                if env.t % env.feedback_every == 0 or done:\n",
    "                    feedback, true_reward = env.get_feedback()\n",
    "                    traj[\"human_rewards\"].append(feedback)\n",
    "                    traj[\"true_rewards\"].append(true_reward)\n",
    "\n",
    "                    current_step_reward = feedback - previous_feedback\n",
    "                    previous_feedback = feedback\n",
    "                else:\n",
    "                    current_step_reward = 0.0\n",
    "\n",
    "                traj[\"step_rewards\"].append(current_step_reward)\n",
    "\n",
    "            true_rewards_this_iter.append(env.get_feedback()[1])\n",
    "\n",
    "            traj[\"steps\"] = env.t\n",
    "            traj[\"coins\"] = env.collected\n",
    "            \n",
    "            # reward to go\n",
    "            returns = []\n",
    "            G_t = 0\n",
    "            for r in reversed(traj[\"step_rewards\"]):\n",
    "                G_t = r + env.gamma * G_t\n",
    "                returns.insert(0, G_t)\n",
    "            \n",
    "            rollout_trajectories.append((traj, returns))\n",
    "                \n",
    "        ## now with these m rollouts, approximate the expectation of estimated reward under policy pi\n",
    "        grad_theta = np.zeros_like(policy.theta)\n",
    "\n",
    "        all_returns = [ret for _, returns in rollout_trajectories for ret in returns]\n",
    "        b = float(np.mean(all_returns))  \n",
    "\n",
    "        for traj, returns in rollout_trajectories:\n",
    "            \n",
    "            # Iterate through (s,a G_t)\n",
    "            for state, action, G_t in zip(traj[\"states\"], traj[\"actions\"], returns):\n",
    "                \n",
    "                # Calculate Advantage\n",
    "                advantage = G_t - b\n",
    "\n",
    "                s_idx, grad_row = policy.grad_log_prob(state, action)\n",
    "                \n",
    "                # Update gradient for this specific step\n",
    "                grad_theta[s_idx] += grad_row * advantage\n",
    "\n",
    "\n",
    "\n",
    "        grad_theta = grad_theta/len(rollout_trajectories)\n",
    "        theta_new = theta_old + eta*grad_theta\n",
    "        theta_old = theta_new.copy()\n",
    "        policy.theta = theta_new\n",
    "        \n",
    "        # logging\n",
    "        avg_true_rewards.append(np.mean(true_rewards_this_iter))\n",
    "        avg_coins.append(np.mean([traj[\"coins\"] for traj,_ in rollout_trajectories]))\n",
    "        avg_length.append(np.mean([traj[\"steps\"] for traj,_ in rollout_trajectories]))\n",
    "\n",
    "        # print(f\"Iter {g}: true={avg_true_rewards[-1]:.2f}, coins={avg_coins[-1]:.2f}, length={avg_length[-1]:.2f}\")\n",
    "\n",
    "    \n",
    "    return policy, avg_true_rewards, avg_coins, avg_length\n",
    "\n",
    "\n",
    "feedbacks = [1, 5, 10, 15, 25, 50]\n",
    "noise = 0.5\n",
    "num_seeds = 5\n",
    "\n",
    "all_results = {fb: [] for fb in feedbacks}\n",
    "\n",
    "for fb in feedbacks:\n",
    "    for seed in range(num_seeds):\n",
    "        print(f\"Training with feedback every {fb} steps, seed {seed}\")\n",
    "        _, avg_true_rewards, _, avg_length = train(\n",
    "            feedback_every=fb,\n",
    "            noise=noise,\n",
    "            seed=seed\n",
    "        )\n",
    "        all_results[fb].append(avg_true_rewards)   # store curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_curves = {}\n",
    "std_curves = {}\n",
    "\n",
    "for fb in feedbacks:\n",
    "    arr = np.array(all_results[fb])   # shape (seeds, T)\n",
    "    mean_curves[fb] = np.mean(arr, axis=0)\n",
    "    std_curves[fb]  = np.std(arr, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(feedbacks)))\n",
    "\n",
    "for idx, fb in enumerate(feedbacks):\n",
    "    mean_curve = mean_curves[fb]\n",
    "    smooth_mean = savgol_filter(mean_curve, 51, 3)\n",
    "\n",
    "    plt.plot(\n",
    "        smooth_mean,\n",
    "        linewidth=2,\n",
    "        color=colors[idx],\n",
    "        label=f\"feedback_every={fb}\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"True Reward\")\n",
    "plt.title(f\"Averaged Learning Curves Across {num_seeds} Seeds, Noise={noise}\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "595",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
